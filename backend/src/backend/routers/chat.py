# backend/src/backend/routers/chat.py
import logging
from fastapi import APIRouter, HTTPException
from backend.models.chat import ChatRequest, ChatResponse
from backend.llm.orchestrator import ChatOrchestrator

# Set up logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# FastAPI router
router = APIRouter()

# Single orchestrator instance reused across requests
print("routers\chat.py initialize orchestrator = ChatOrchestrator()")
orchestrator = ChatOrchestrator(model_name="Qwen3:4b")
print("routers\chat.py done initialize orchestrator = ChatOrchestrator()")

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Orchestrated chat endpoint.

    Workflow:
    1. Sends user query to LLM through ChatOrchestrator.
    2. LLM decides if a backend tool (MCP server) is required.
    3. Calls the appropriate tool via MCPManager if needed.
    4. Returns the final answer generated by LLM, along with optional tool output.

    Args:
        request (ChatRequest): User query wrapped in Pydantic model.

    Returns:
        ChatResponse: Contains the final answer string from LLM.

    Raises:
        HTTPException: If any step in orchestration fails.
    """
    logger.info(f"Received chat request: {request.message}")
    print("routers\chat.py ",request.message)

    try:
        # Process the user query using the orchestrator
        result = await orchestrator.process_query(request.message)
        logger.info(f"Orchestrator raw result: {result}")

        # result is already a plain string from the orchestrator
        response_text = result
        logger.info(f"Returning chat response: {response_text}")

        return ChatResponse(response=response_text)

    except Exception as e:
        logger.error(f"Chat orchestration failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Chat orchestration failed: {str(e)}"
        )
