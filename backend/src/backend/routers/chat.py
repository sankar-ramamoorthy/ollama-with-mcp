# backend/src/backend/routers/chat.py
import logging
from fastapi import APIRouter, HTTPException
from backend.models.chat import ChatRequest, ChatResponse
from backend.llm.orchestrator import LLMOrchestrator

# Set up logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# FastAPI router
router = APIRouter()

# Single orchestrator instance reused across requests
orchestrator = LLMOrchestrator()

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Orchestrated chat endpoint.

    Workflow:
    1. Sends user query to LLM through LLMOrchestrator.
    2. LLM decides if a backend tool (MCP server) is required.
    3. Calls the appropriate tool via MCPManager if needed.
    4. Returns the final answer generated by LLM, along with optional tool output.

    Args:
        request (ChatRequest): User query wrapped in Pydantic model.

    Returns:
        ChatResponse: Contains the final answer string from LLM.

    Raises:
        HTTPException: If any step in orchestration fails.
    """
    logger.info(f"Received chat request: {request.message}")

    try:
        # Process the user query using the orchestrator
        result = await orchestrator.process_query(request.message)
        response_text = result.get("response", "No response from LLM")
        tool_output = result.get("tool_output")
        if tool_output:
            logger.info(f"Tool output: {tool_output}")
        logger.info(f"Returning chat response: {response_text}")
        return ChatResponse(response=response_text)

    except Exception as e:
        logger.error(f"Chat orchestration failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Chat orchestration failed: {str(e)}"
        )
