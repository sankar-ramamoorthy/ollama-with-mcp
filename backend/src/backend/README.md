# **Backend Service (`backend/src/backend`)**

This directory contains the full **FastAPI backend**, including:

* The `/chat` API entrypoint
* The **LLM Orchestrator** (Ollama reasoning + tool planning)
* The **MCP Client Manager** (tool execution through MCP servers)
* Routers, services, models, and tests

The backend acts as the central controller that connects the **UI â†’ LLM â†’ MCP tools**.

---

# ğŸ“¦ **Directory Structure**

```
backend/src/backend/
â”œâ”€â”€ app.py                  # FastAPI app entrypoint
â”œâ”€â”€ llm/                    # LLM logic + tool orchestration
â”‚   â”œâ”€â”€ orchestrator.py     # Core tool/LLM pipeline
â”‚   â”œâ”€â”€ prompt_templates.py # System + tool prompting logic
â”‚   â”œâ”€â”€ ollama_service.py   # Ollama HTTP client
â”‚   â”œâ”€â”€ schemas.py          # Internal LLM models
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ mcp/                    # MCP client integration layer
â”‚   â””â”€â”€ manager.py          # Multi-MCP server manager
â”œâ”€â”€ models/                 # Pydantic request/response models
â”‚   â”œâ”€â”€ chat.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ routers/                # FastAPI endpoints
â”‚   â”œâ”€â”€ chat.py             # /chat endpoint (main entrypoint)
â”‚   â”œâ”€â”€ datetime.py         # (Optional) direct testing routes
â”‚   â”œâ”€â”€ geocoding.py
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ weather.py
â”‚   â”œâ”€â”€ health.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ services/               # Backend utility services
â”‚   â”œâ”€â”€ chat_service.py     # Bridges API requests â†’ orchestrator
â”‚   â”œâ”€â”€ ollama_service.py   # (Deprecated - see llm/ollama_service.py)
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ tests/                  # pytest test suite
```

---

# ğŸš€ **What the Backend Does**

The backend performs **three major roles**:

---

## **1. API Layer (FastAPI)**

Main endpoints:

| Endpoint                                         | Purpose                               |
| ------------------------------------------------ | ------------------------------------- |
| `POST /chat`                                     | Primary chat interface used by Gradio |
| `GET /health`                                    | Container health check                |
| (Optional) `/weather`, `/geocoding`, `/datetime` | Direct testing/micro-endpoints        |

These map external client requests into an internal **Chat Service** which then calls the LLM orchestrator.

---

## **2. LLM Layer (Ollama Orchestrator)**

Located in `llm/orchestrator.py`.

This component:

1. Sends user messages to Ollama
2. Lets the model decide **whether a tool call is needed**
3. If yes â†’ calls MCP Manager
4. Provides MCP tool results back to the LLM
5. LLM generates the final response

Tool-thinking prompts and formatting are defined in:

```
llm/prompt_templates.py
```

The LLM is accessed through:

```
llm/ollama_service.py
```

---

## **3. MCP Manager Layer**

Located in:

```
mcp/manager.py
```

The manager:

* Connects to each MCP server (Datetime, DDGS, Geocoding, Weather)
* Caches available tools + schemas
* Executes tool calls requested by the LLM
* Normalizes and returns structured JSON results

This abstraction hides the complexity of:

* HTTP vs WebSocket MCP transports
* Tool schemas and argument validation
* Multi-server tool discovery

---

# ğŸ”§ **How /chat Works Internally**

### 1ï¸âƒ£ **User â†’ Frontend â†’ Backend**

Frontend sends:

```json
{
  "message": "What's the weather in Tokyo?",
  "history": [...]
}
```

---

### 2ï¸âƒ£ **Backend â†’ LLM (Planning)**

Ollama receives instructions + context via orchestrator:

```
System: You may call tools when needed.
User: What's the weather in Tokyo?
```

LLM responds with either:

* A reasoning + tool call request
* OR a natural-language answer

---

### 3ï¸âƒ£ **If a Tool Call is Needed**

Example reasoning:

```
<tool>
{
   "name": "get_weather",
   "arguments": { "location": "Tokyo" }
}
</tool>
```

Backend passes this to:

```
mcp/manager.py
```

---

### 4ï¸âƒ£ **MCP Manager â†’ Weather-MCP Server**

Weather-MCP internally calls **Geocoding-MCP â†’ Open-Meteo API**.

Returns structured JSON:

```json
{
  "temp_c": 17.3,
  "description": "Clear sky"
}
```

---

### 5ï¸âƒ£ **Backend â†’ LLM (Final Answer)**

LLM receives tool results and generates natural language:

> "The current weather in Tokyo is 17Â°C with clear skies."

---

### 6ï¸âƒ£ **Backend â†’ Frontend**

JSON returned to UI.

---

# ğŸ‹ **Running the Backend (Standalone)**

### Using Docker Compose (recommended)

```bash
docker compose up backend
```

### Running locally

```bash
cd backend
uv sync
uv run uvicorn backend.app:app --reload --host 0.0.0.0 --port 8000
```

---

# ğŸ“˜ Development Notes

### âœ” Backend expects:

* **Docker + Ollama running**
* At least one model pulled (e.g. `qwen2.5`, `qwen3:4b`)

### âœ” MCP servers must be reachable:

```
datetime-mcp   â†’ 50051
ddgs-mcp       â†’ 50052
weather-mcp    â†’ 50053
geocoding-mcp  â†’ 50054
```

### âœ” Frontend requires only:

* Backend reachable at: `http://backend:8000/chat`

---

# ğŸ§ª Tests

Tests live in:

```
backend/src/backend/tests/
```

They verify:

* `/chat` endpoint behavior
* MCP integration (mocked)
* Orchestrator planning logic
* Health checks

---

# ğŸ“„ License

This backend is part of the main project repository and follows the project-wide license.

