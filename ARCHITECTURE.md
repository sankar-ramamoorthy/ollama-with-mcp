# ğŸ—ï¸ **ARCHITECTURE.md**

# **System Architecture Overview**

This project implements a **full MCP-enabled LLM application stack** using:

* **FastAPI backend** with LLM orchestration
* **Gradio frontend**
* **Multiple MCP servers** (Datetime, DDGS Search, Geocoding, Weather)
* **Ollama ( Qwen3:4b)** as the reasoning engine
* **Docker Compose** for orchestration

The architecture is modular, tool-driven, and cleanly layered.

---

# ğŸ”· High-Level Diagram

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚        Gradio UI         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  HTTP /chat
                            â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ FastAPI Backend          â”‚
             â”‚ - MCP Client Manager     â”‚
             â”‚ - LLM Orchestrator       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  MCP Calls
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                     â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DDGS-MCP       â”‚   â”‚ Weather-MCP    â”‚         â”‚ Datetime-MCP     â”‚
â”‚ (DuckDuckGo)   â”‚   â”‚ (Open-Meteo)   â”‚         â”‚ (UTC datetime)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ (depends on)
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Geocoding-MCP      â”‚
                   â”‚ (Nominatim)        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Ollama LLM     â”‚
            â”‚  (Granite/Qwen3)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ”· System Components

## 1. **Gradio Frontend**

* Runs at **port 7860**
* Communicates only with backend (`/chat`)
* Maintains chat history and logs
* Displays final LLM responses

---

## 2. **FastAPI Backend**

The backend makes all decisions and orchestrates the full system.

### Internal responsibilities:

* **/chat endpoint** (main entrypoint)
* **LLM Orchestrator**

  * Calls Ollama for planning and final answer generation
* **MCP Manager**

  * Discovers tools
  * Calls the correct MCP server automatically
  * Returns JSON results back to the orchestrator

### Files:

```
backend/src/backend/
â””â”€â”€ llm/
    â”œâ”€â”€ orchestrator.py
    â”œâ”€â”€ prompt_templates.py
    â”œâ”€â”€ ollama_service.py
â””â”€â”€ mcp/manager.py
â””â”€â”€ routers/chat.py
```

### Data flow:

1. User sends message â†’ FastAPI
2. LLM decides if a tool is required
3. Orchestrator calls MCP tool
4. MCP server performs external API logic
5. Backend gives results to LLM
6. LLM creates final message
7. Backend returns response to UI

---

## 3. **MCP Servers (Tools)**

### ğŸŸ¦ **DDGS-MCP (DuckDuckGo search)**

* Tool: `ddgs_search(query)`
* Fast, no API key required
* Used for general web lookups

### ğŸŸ© **Weather-MCP**

* Tool: `get_weather(location)`
* Uses **Geocoding-MCP** first
* Uses **Open-Meteo** for realtime weather

### ğŸŸ¨ **Geocoding-MCP**

* Tool: `geocode(location)`
* Uses **Nominatim OpenStreetMap**
* Returns lat/lon for weather

### ğŸŸ§ **Datetime-MCP**

* Tool: `get_current_datetime()`
* Returns current UTC timestamp

### Server File Pattern:

```
mcp-servers/<name>/<name>_mcp/
   â”œâ”€â”€ server.py  (@mcp.tool)
   â””â”€â”€ tool.py    (business logic)
```

---

## 4. **Ollama LLM**

* Local inference server on port **11434**
* Models: **Granite**, **Qwen3:4b**, or any supported model
* Handles:

  * Planning ("should I call a tool?")
  * Final natural output after tool results

---

# ğŸ”· Detailed Data Flow

Below is the exact sequence for a full tool-enabled chat request.

| Step                      | Source â†’ Destination                | Description                                    |
| ------------------------- | ----------------------------------- | ---------------------------------------------- |
| **1. User Input**         | User â†’ Gradio                       | User types a message.                          |
| **2. API Call**           | Gradio â†’ FastAPI `/chat`            | Sends JSON with message + history.             |
| **3. LLM Planning**       | FastAPI â†’ Orchestrator â†’ Ollama     | LLM decides whether a tool is necessary.       |
| **4. Tool Decision**      | LLM â†’ MCP Manager                   | Orchestrator reads tool instructions from LLM. |
| **5. MCP Execution**      | MCP Manager â†’ MCP Server            | Backend calls the correct MCP server.          |
| **6. External API Calls** | MCP Server â†’ External API           | DDGS, Nominatim, Open-Meteo, etc.              |
| **7. Tool Response**      | MCP Server â†’ MCP Manager            | MCP returns structured JSON.                   |
| **8. LLM Finalization**   | MCP Manager â†’ Orchestrator â†’ Ollama | LLM integrates tool results into final answer. |
| **9. Backend Response**   | FastAPI â†’ Gradio                    | JSON response returned to UI.                  |
| **10. UI Display**        | Gradio                              | Chat window shows assistant message.           |

---

# ğŸ”· Port Mapping

| Service               | Port      | Notes                      |
| --------------------- | --------- | -------------------------- |
| **FastAPI Backend**   | **8000**  | Main API                   |
| **Gradio Frontend**   | **7860**  | UI                         |
| **Datetime-MCP**      | **50051** | MCP                        |
| **DDGS-MCP**          | **50052** | MCP                        |
| **Weather-MCP**       | **50053** | MCP                        |
| **Geocoding-MCP**     | **50054** | MCP                        |
| **SearchXNG Service** | **8181**  | SearxNG backend (optional) |
| **Ollama**            | **11434** | LLM inference              |

---

# ğŸ”· Component Responsibility Matrix

| Component            | Category          | Responsibilities                              |
| -------------------- | ----------------- | --------------------------------------------- |
| **Gradio UI**        | Frontend          | Render chat UI, send requests, show responses |
| **FastAPI Backend**  | Application Layer | Routes requests, orchestrates LLM + tools     |
| **Chat Router**      | API               | `/chat` endpoint                              |
| **LLM Orchestrator** | AI Logic          | Tool planning, final answer generation        |
| **Prompt Templates** | AI Logic          | System + tool instructions                    |
| **MCP Manager**      | Coordination      | Calls MCP servers, returns JSON               |
| **Datetime-MCP**     | Tool              | Current UTC datetime                          |
| **DDGS-MCP**         | Tool              | DuckDuckGo Web Search                         |
| **Geocoding-MCP**    | Tool              | Convert text â†’ coordinates                    |
| **Weather-MCP**      | Tool              | Current weather                               |
| **Nominatim**        | External          | Geocoding backend                             |
| **Open-Meteo**       | External          | Weather backend                               |
| **Ollama**           | LLM               | Model reasoning + output                      |

---

# ğŸ”· Summary

This architecture provides:

### âœ… Modular MCP-based tools

### âœ… Local LLM with tool reasoning

### âœ… Chained tools: DDGS â†’ Geocoding â†’ Weather

### âœ… Clean separation between Frontend, Backend, Tools

### âœ… Dockerized and production-ready

It is designed to scale as more MCP servers or LLM backends are added.

---
