
**Core Flow:** `curl /chat` â†’ `orchestrator.py` â†’ **LLM decides tool** â†’ `FastMCP Client` â†’ **MCP Server** â†’ **LLM formats answer**
```

### ** `README.md` **

```markdown
# ğŸ§  Ollama + MCP Agent System  
**A fully local, framework-free, ReAct-style AI agent powered by modular MCP microservices.**

This project demonstrates how to build **real agentic AI systems** using only:

- **Local LLMs (Ollama + Qwen3:4B)**  
- **FastMCP microservices (datetime, weather, geocoding, search)**  
- **FastAPI**  
- **Docker Compose**

No LangChain.  
No cloud LLMs.  
No closed-source toolchains.

Just **a clean, transparent,  agent architecture** that anyone can run locally.

Modular AI Agent platform using **Ollama (Qwen3:4B)** + **FastMCP tools** + **FastAPI** + **Docker Compose**.

**LLM intelligently delegates to specialized MCP microservices** (datetime, weather, search, geocoding).


# ğŸ“Œ Prerequisites

Before running the system, you must have:

### âœ” **Docker** installed and running  
https://www.docker.com/get-started/

### âœ” **Ollama** installed and running  
https://ollama.com/download

### âœ” **Qwen3:4B model already pulled in Ollama**
```bash
ollama pull qwen3:4b

ğŸ§© What Kind of Agent Is This?

This system implements a ReAct-style, tool-using LLM agent, also known as a Toolformer-style single-step agent.

Agent Loop:
Thought â†’ Tool Call â†’ Observation â†’ Final Answer

LLM decides if a tool is needed
Orchestrator routes the call to the correct MCP server
Tool returns structured JSON
LLM synthesizes the final natural-language answer

ğŸ”§ Core Architecture

Flow:
User â†’ FastAPI /chat â†’ LLM â†’ MCP Tool â†’ LLM synthesis â†’ Response
User
  â†“
FastAPI /chat
  â†“
LLM (Ollama + Qwen3)
  â†“ decides tool
Orchestrator â†’ FastMCP Client â†’ MCP Tool Server
  â†“ tool result
LLM formats final answer
  â†“
Response

Each tool is its own isolated MCP microservice running in Docker.
ğŸ¯ Features
ğŸ¤– Autonomous tool-using agent (LLM chooses which MCP tool to call)
ğŸ§© Microservice architecture using MCP
ğŸ•¸ï¸ Search tool via DuckDuckGo MCP 
ğŸŒ¦ï¸ Weather tool using Open-Meteo
ğŸŒ Geocoding tool using Nominatim
ğŸ—“ï¸ Datetime tool returning the current UTC date/time
ğŸ¨ Gradio UI interface
ğŸ³ Fully Dockerized

ğŸ”’ 100% local; no cloud LLMs
## ğŸ¯ Features

- âœ… **Chat Orchestrator**: LLM decides tool â†’ executes â†’ formats natural response
- âœ… **Datetime MCP**: "What's today's date?" â†’ `datetime-mcp:50051`
- âœ… **Weather MCP**: "Weather in Dallas?" â†’ `weather-mcp:50053` â†’ Open-Meteo
- âœ… **Search MCP**: DDGS  integration (port 50052)
- âœ… **Geocoding MCP**: Weather geocoding support (port 50054)
- ğŸ¨ **Gradio UI**: `http://localhost:7860`
- ğŸš€ **Production-ready**: Dockerized, uv dependency management


| Service              | Port  | Status | Description                                    |
| -------------------- | ----- | ------ | ---------------------------------------------- |
| Backend Orchestrator | 8000  | âœ…      | FastAPI + LLM tool decision + MCP execution    |
| Datetime MCP         | 50051 | âœ…      | "What's today's date?"â†’ UTC ISO datetime       |
| Weather MCP          | 50053 | âœ…      | "Weather in Dallas?"â†’ Open-Meteo via geocoding |
| Geocoding MCP        | 50054 | âœ…      | Address â†’ lat/lon (Nominatim, used by weather) |
| Search MCP           | 50052 | âš ï¸     |  DDGS Duck Duck Go Search            |
| Gradio UI            | 7860  | âœ…      | Web interface                                  |

## ğŸš€ Quick Start

```
# Clone & start
git clone <repo>
cd ollama-with-mcp
docker compose up -d

# Backend API ready
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is today'\''s date?"}'
```

**Response:** `"Today's date is November 30, 2025."`

## ğŸ› ï¸ API Endpoints

| Endpoint | Purpose | Example |
|----------|---------|---------|
| `POST /chat` | **Main orchestrator** | `{"message": "Weather in Dallas?"}` |
| `POST /datetime/get` | Direct datetime | Returns UTC ISO datetime |
| `POST /weather` | Direct weather | `{"location": "Dallas"}` |
| `GET /health` | System status | Health check all services |

## ğŸ—ï¸ Architecture

```
User â†’ FastAPI (/chat) â†’ LLM Decision â†’ MCP Tool â†’ LLM Synthesis â†’ Response
                    â†“
              orchestrator.py orchestrates it all
```

**See [DIRECTORY_STRUCTURE.md](DIRECTORY_STRUCTURE.md) for details.**


## ğŸ³ Docker Compose Services

| Service | Port | Purpose |
|---------|------|---------|
| `backend` | 8000 | FastAPI orchestrator |
| `datetime-mcp` | 50051 | Date/time tool âœ… |
| `weather-mcp` | 50053 | Weather tool âœ… |
| `ddgs` | 50052 | Web search  |
| `frontend` | 7860 | Gradio UI |


## ğŸ“š Development

```
# Backend (uv)
cd backend
uv sync
uv run pytest

# MCP Servers (uv)  
cd mcp-servers/datetime
uv sync
uv run python datetime_mcp/server.py
```

## ğŸ™ Acknowledgments

- [FastMCP](https://github.com/jlowin/fastmcp) - MCP servers
- [Ollama](https://ollama.com) - Local LLM (Qwen3:4B )
- [DuckDuckGo Search](https://pypi.org/project/duckduckgo-search/) - Web search
- [Open-Meteo](https://open-meteo.com) - Weather API
- [Nominatim](https://nominatim.org/) â€” OpenStreetMap geocoding service
```

