# **Ollama + MCP Agent System**


Modular AI Agent platform using **Ollama (Qwen3:4B)** + **FastMCP tools** + **FastAPI** + **Docker Compose**.

**A fully local, framework-free ReAct-style AI agent using Ollama, FastAPI, and MCP microservices (weather, search, geocoding, datetime).**

---

## ğŸ¯ Features

* âœ… **Chat Orchestrator**: LLM decides tool â†’ executes â†’ formats natural response
* ğŸ—“ï¸ **Datetime MCP**: Returns the **current UTC date & time** (ISO 8601)
* ğŸŒ¤ï¸ **Weather MCP**: "Weather in Dallas?" â†’ `weather-mcp:50053` (Open-Meteo)
* ğŸ” **Search MCP**:  DDGS (port 50052)
* ğŸ—ºï¸ **Geocoding MCP**: Uses Nominatim to resolve locations (port 50054)
* ğŸ¨ **Gradio UI**: `http://localhost:7860`
* ğŸ³ **Dockerized system**: Each tool runs as an MCP microservice
* ğŸ”’ **Fully local**: Requires **Ollama + Qwen3:4B** pre-installed

---

## ğŸ“¦ Requirements

* **Docker & Docker Compose** (installed and running)
* **Ollama installed and running**
* **Qwen3:4B pulled locally:**

```
ollama pull qwen2.5:4b
```

---

## ğŸš€ Quick Start

```
# Clone & start
git clone <repo>
cd ollama-with-mcp
docker compose up -d

# Backend API ready
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is today'\''s date?"}'
```

**Response:**
`"Today's date is November 30, 2025."`

---

## ğŸ› ï¸ API Endpoints

| Endpoint             | Purpose           | Example                             |
| -------------------- | ----------------- | ----------------------------------- |
| `POST /chat`         | Main orchestrator | `{"message": "Weather in Dallas?"}` |
| `POST /datetime/get` | Direct datetime   | Returns UTC ISO datetime            |
| `POST /weather`      | Direct weather    | `{"location": "Dallas"}`            |
| `GET /health`        | System status     | Health check                        |
| `POST /search/get`   | Web Search        | `{"message": "News in Dallas?"}`    |


---

## ğŸ—ï¸ Architecture

```
User â†’ FastAPI (/chat) â†’ LLM Decision â†’ MCP Tool â†’ LLM Synthesis â†’ Response
                    â†“
              orchestrator.py orchestrates it all
```

---

## ğŸ³ Docker Compose Services

| Service       | Port  | Purpose                 |
| ------------- | ----- | ----------------------- |
| backend       | 8000  | FastAPI orchestrator    |
| datetime-mcp  | 50051 | Date/time MCP           |
| weather-mcp   | 50053 | Weather MCP             |
| ddgs-mcp      | 50052 | Web search MCP          |
| geocoding-mcp | 50054 | Nominatim geocoding MCP |
| frontend      | 7860  | Gradio UI               |

---

## ğŸ“š Development

```
# Backend (uv)
cd backend
uv sync
uv run pytest

# MCP Servers (uv)  
cd mcp-servers/datetime
uv sync
uv run python datetime_mcp/server.py
```

---

## ğŸ™ Acknowledgments

* [FastMCP](https://github.com/jlowin/fastmcp) â€“ MCP servers
* [Ollama](https://ollama.com) â€“ Local LLM
* [DuckDuckGo Search](https://pypi.org/project/duckduckgo-search/) â€“ Search 
* [Nominatim](https://nominatim.org/) â€“ OpenStreetMap geocoding
* [Open-Meteo](https://open-meteo.com) â€“ Weather API

---
